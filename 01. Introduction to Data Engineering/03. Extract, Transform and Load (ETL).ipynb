{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Row-oriented databases and OLTP go hand-in-hand.\n",
    "- Column-oriented databases and OLAP go hand-in-hand.\n",
    "- OLTP means the system is optimized for transactions.\n",
    "- APIs mostly use a more structured form of data, for example JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the requests module to get the Hacker News post's JSON object.\n",
    "- Print out the response, parsed as a JSON.\n",
    "- Parsing as JSON again, assign the \"score\" key of the post to post_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by': 'neis', 'descendants': 0, 'id': 16222426, 'score': 17, 'time': 1516800333, 'title': 'Duolingo-Style Learning for Data Science: DataCamp for Mobile', 'type': 'story', 'url': 'https://medium.com/datacamp/duolingo-style-learning-for-data-science-datacamp-for-mobile-3861d1bc02df'}\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the Hackernews post\n",
    "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
    "\n",
    "# Print the response parsed as JSON\n",
    "print(resp.json())\n",
    "\n",
    "# Assign the score of the test to post_score\n",
    "post_score = resp.json()[\"score\"]\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the extract_table_to_pandas() function definition to include the tablename argument within the query.\n",
    "- Fill in the connection URI. The username and password are repl and password, respectively. The host is localhost and port is 5432. The database is pagila.\n",
    "- Complete the function calls of extract_table_to_pandas() to extract the film and customer tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sqlalchemy\n",
    "# import pandas as pd\n",
    "\n",
    "# # Function to extract table to a pandas DataFrame\n",
    "# def extract_table_to_pandas(tablename, db_engine):\n",
    "#     query = \"SELECT * FROM {}\".format(tablename)\n",
    "#     return pd.read_sql(query, db_engine)\n",
    "\n",
    "# # Connect to the database using the connection URI\n",
    "# connection_uri = \"postgresql://repl:password@localhost:5432/pagila\" \n",
    "# db_engine = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# # Extract the film table into a pandas DataFrame\n",
    "# extract_table_to_pandas(\"film\", db_engine)\n",
    "\n",
    "# # Extract the customer table into a pandas DataFrame\n",
    "# extract_table_to_pandas(\"customer\", db_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the .astype() method to convert the rental_rate column into a column of string objects, and assign the results to rental_rate_str.\n",
    "- Split rental_rate_str on '.' and expand the results into columns. Assign the results to rental_rate_expanded.\n",
    "- Assign the newly created columns into films_df using the column names rental_rate_dollar and rental_rate_cents respectively, setting them to the expanded version using the appropriate index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the rental rate column as a string\n",
    "# rental_rate_str = film_df.rental_rate.astype(\"str\")\n",
    "\n",
    "# # Split up and expand the column\n",
    "# rental_rate_expanded = rental_rate_str.str.split(\".\", expand=True)\n",
    "\n",
    "# # Assign the columns to film_df\n",
    "# film_df = film_df.assign(\n",
    "#     rental_rate_dollar =rental_rate_expanded[0],\n",
    "#     rental_rate_cents =rental_rate_expanded[1],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can do transformations using PySpark, you need to get the data into the Spark framework. You saw how to do this using PySpark. \n",
    "\n",
    "```\n",
    "spark.read.jdbc(\"jdbc:postgresql://localhost:5432/pagila\",\n",
    "                \"customer\",\n",
    "                {\"user\":\"repl\",\"password\":\"password\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take the mean rating per film_id, and assign the result to ratings_per_film_df.\n",
    "- Complete the .join() statement to join on the film_id column.\n",
    "- Show the first 5 results of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use groupBy and mean to aggregate the column\n",
    "# ratings_per_film_df = rating_df.groupBy('film_id').mean('film_id')\n",
    "\n",
    "# # Join the tables using the film_id column\n",
    "# film_df_with_ratings = film_df.join(\n",
    "#     ratings_per_film_df,\n",
    "#     film_df.film_id==ratings_per_film_df.film_id\n",
    "# )\n",
    "\n",
    "# # Show the 5 first results\n",
    "# print(film_df_with_ratings.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You saw that there's a difference between OLAP and OLTP operations. A small recap:\n",
    "\n",
    "- OLAP: Online analytical processing\n",
    "- OLTP: Online transaction processing\n",
    "- It's essential to use the right database for the right job. \n",
    "- Typically, analytical databases are column-oriented.\n",
    "- Massively parallel processing (MPP) databases are usually column-oriented.\n",
    "- Databases optimized for OLAP are usually not great at OLTP operations.\n",
    "- Analytical and application databases have different use cases and should be separated if possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write the pandas DataFrame film_pdf to a parquet file called \"films_pdf.parquet\".\n",
    "- Write the PySpark DataFrame film_sdf to a parquet file called \"films_sdf.parquet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the pandas DataFrame to parquet\n",
    "# film_pdf.to_parquet(\"films_pdf.parquet\")\n",
    "\n",
    "# # Write the PySpark DataFrame to parquet\n",
    "# film_sdf.write.parquet(\"films_sdf.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the connection URI for to create the database engine. The user and password are repl and password respectively. The host is localhost, and the port is 5432. This time, the database is dwh.\n",
    "- Finish the call so we use the \"store\" schema in the database. If the table exists, replace it completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finish the connection URI\n",
    "# connection_uri = \"postgresql://repl:password@localhost:5432/dwh\"\n",
    "# db_engine_dwh = sqlalchemy.create_engine(connection_uri)\n",
    "\n",
    "# # Transformation step, join with recommendations data\n",
    "# film_pdf_joined = film_pdf.join(recommendations)\n",
    "\n",
    "# # Finish the .to_sql() call to write to store.film\n",
    "# film_pdf_joined.to_sql(\"film\", db_engine_dwh, schema=\"store\", if_exists=\"replace\")\n",
    "\n",
    "# # Run the query to fetch the data\n",
    "# pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", db_engine_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the etl() function by making use of the functions defined in the exercise description.\n",
    "- Make sure etl_task uses the etl callable.\n",
    "- Set up the correct upstream dependency. Note that etl_task should wait for wait_for_table to be finished.\n",
    "- The sample code contains a sample run. This means the ETL pipeline runs when you run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the ETL function\n",
    "# def etl():\n",
    "#     film_df = extract_film_to_pandas()\n",
    "#     film_df = transform_rental_rate(film_df)\n",
    "#     load_dataframe_to_film(film_df)\n",
    "\n",
    "# # Define the ETL task using PythonOperator\n",
    "# etl_task = PythonOperator(task_id='etl_film',\n",
    "#                           python_callable=etl,\n",
    "#                           dag=dag)\n",
    "\n",
    "# # Set the upstream to wait_for_table and sample run etl()\n",
    "# etl_task.set_upstream(wait_for_table)\n",
    "# etl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to move the dag.py file containing the DAG you defined in the previous exercise to, the DAGs folder. Here are the steps to find it:\n",
    "\n",
    "- The airflow home directory is defined in the AIRFLOW_HOME environment variable. Type echo $AIRFLOW_HOME to find out.\n",
    "\n",
    "- In this directory, find the airflow.cfg file. Use head to read the file, and find the value of the dags_folder.\n",
    "\n",
    "Now you can find the folder and move the dag.py file there: mv ./dag.py <dags_folder>.\n",
    "\n",
    "Which files does the DAGs folder have after you moved the file?\n",
    "\n",
    "- It has two DAG files: dag.py and dag_recommendations.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`0 0 * * *` means:\n",
    "- Daily at midnight.it runs at 0:00, so at midnight."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
